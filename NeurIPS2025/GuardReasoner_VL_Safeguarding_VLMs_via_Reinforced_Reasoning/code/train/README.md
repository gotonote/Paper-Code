## Training Pipeline of GuardReasoner-VL

### Step 1: Reasoning Data Synthesis
We release the synthesized reasoning data [GuardReasoner-VLTrain](https://huggingface.co/datasets/yueliu1999/GuardReasoner-VLTrain). Use it directly.

If you plan to synthesize the reasoning steps on your own data, run the following code.

```
export OPENAI_API_KEY=your_openai_api_key

python reasoning_data_synthesis.py
```


### Step 2: Reasoning SFT
1. Prepare the rasoning data for R-SFT.

    ```
    python prepare_data_rsft.py
    ```
2. Move `GuardReasoner-VLTrainImage.json`, `GuardReasoner-VLTrainText.json`, `GuardReasoner-VLTrainTextImage.json` to data folder in LLaMA-Factory and configre `dataset_info.json` as follows.

    ```
    "GuardReasoner_VLTrainImage": {
      "file_name": "GuardReasoner-VLTrainImage.json"
    },
    "GuardReasoner_VLTrainText": {
      "file_name": "GuardReasoner-VLTrainText.json"
    },
    "GuardReasoner_VLTrainTextImage": {
      "file_name": "GuardReasoner-VLTrainTextImage.json"
    },
    ```
3. Download the image data.
   ```
   git clone https://huggingface.co/datasets/yueliu1999/GuardReasoner-VLTrain-Image
   cd ./GuardReasoner-VLTrain-Image/
   mv image.zip /your_path/LLaMA-Factory/data/
   mv text_image.zip /your_path/LLaMA-Factory/data/
   unzip image.zip
   unzip text_image.zip
   ```
4. Run R-SFT via [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory). The scripts are provided.  
    ```
    bash r_sft_3b.sh
    bash r_sft_7b.sh
    ```
### Step3: Online RL
1. Conduct rejection sampling and data augmentation for the RL training data..
    ```
    bash gen_rl_data_3b.sh
    bash gen_rl_data_7b.sh
    ```

2. Conduct online RL via [EasyR1](https://github.com/hiyouga/EasyR1). 
    ```
    bash rl_3b.sh
    bash rl_7b.sh
    ```

