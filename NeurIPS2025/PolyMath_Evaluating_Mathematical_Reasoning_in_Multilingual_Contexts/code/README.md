<div align="center">

  <h2>
    <img src="ASSETS/pyramid.png" alt="logo" width="25"/>
    PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts
  </h2>

</div>



<p align="center">
  <a href="https://arxiv.org/abs/2504.18428">
    <img src="https://img.shields.io/badge/arXiv-2504.18428-b31b1b.svg?logo=arxiv" alt="arXiv Badge"/>
  </a>
  <a href="https://huggingface.co/datasets/Qwen/PolyMath">
    <img src="https://img.shields.io/badge/HuggingFace-Dataset-yellow?logo=huggingface" alt="Hugging Face Badge"/>
  </a>
  <a href="https://qwen-polymath.github.io/">
    <img src="https://img.shields.io/badge/Leaderboard-Website-brightgreen?logo=trophy" alt="Leaderboard Badge"/>
  </a>
  <a href="./LICENSE">
    <img src="https://img.shields.io/badge/License-Apache 2.0-blue.svg?logo=open-source-initiative" alt="Apache-2.0 License Badge"/>
  </a>
</p>



This is the official repository for the paper **"PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts"**.


## ğŸ“– Introduction

**PolyMath** is a multilingual mathematical reasoning benchmark covering 18 languages and 4 easy-to-hard difficulty levels, with 9,000 high-quality problem samples. Our benchmark ensures difficulty comprehensiveness, language diversity, and high-quality translation, making it a highly discriminative multilingual mathematical benchmark in the era of reasoning LLMs.



## âœ¨ Features

- ğŸ“ˆ **Broad Difficulty Range:** PolyMath defines and partitions **mathematical difficulty across four levels** using two core dimensions: *Thought Depth* and *Knowledge Breadth*, ranging from K-12 to Olympiad and advanced frontier mathematics, with **125 problems per language at each level**.

<div align="center">
  <img src="ASSETS/level.png" alt="logo" width="85%"/>
</div>



- ğŸŒ **Language Diversity:** Each problem in PolyMath is available in **18 parallel language versions**, encompassing over 75% of the worldâ€™s native speakers and major language families, ensuring diversity across both high-resource and low-resource languages.

<div align="center">
  <img src="ASSETS/language.png" alt="logo" width="50%"/>
</div>

- ğŸ§‘â€ğŸ« **High-Quality Annotation:** Each problem translation is **calibrated by language experts**, avoiding direct use of LLM-generated outputs and ensuring precise term and logical clarity.

<div align="center">
  <img src="ASSETS/human.png" alt="logo" width="90%"/>
</div>



## ğŸ› ï¸ Data Usage

The PolyMath dataset is publicly available and can be accessed in [![Hugging Face](https://img.shields.io/badge/Dataset-HuggingFace-yellow?logo=huggingface)](https://huggingface.co/datasets/Qwen/PolyMath), with the following format:


```
PolyMath/
  â”œâ”€â”€ ar/
  â”‚   â”œâ”€â”€ low.parquet
  â”‚   â”œâ”€â”€ medium.parquet
  â”‚   â”œâ”€â”€ high.parquet
  |   â””â”€â”€ top.parquet
  â”œâ”€â”€ bn/
  â”œâ”€â”€ ...
  â””â”€â”€ zh/
```

* Additionally, all prompts used in the inference process are provided in `instruction.py`.


## ğŸ§ª Evaluation

### Environment Preparation

```
conda create -n polymath python=3.10
conda activate polymath
pip install -r requirements.txt
```

### Output Process

Given that varying inference engines may generate outputs in different formats, we request that you standardize your results into the specified format:

```
mkdir output
cd output
```

1. Take `/{model_name}` as the primary directory tier, and `/{difficulty_level}` as the secondary tier.

2. For each language, generate a `{lang_name}.jsonl` file within `/{difficulty_level}`, ensuring it includes 125 output samples. Each sample should adhere to the following format:

```json
{"idx: 0, ...}
...
{
  "idx": 114,    ### unique sample id
  "question": "å‡è®¾åœ¨å¹³é¢ä¸Šçš„ä¸€ä¸ªç´§é›† $C$ æ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼šå¯¹æ¯ä¸€ä¸ªæ–¹å‘ï¼Œéƒ½å­˜åœ¨ä¸€æ¡è¯¥æ–¹å‘ä¸Šçš„ç›´çº¿ $l$ï¼Œä½¿å¾— $l \\cap C$ çš„ç»´æ•°è‡³å°‘ä¸º $\\frac{1}{2}$ã€‚é‚£ä¹ˆï¼Œ$C$ çš„æœ€å°å¯èƒ½ç»´æ•°æ˜¯å¤šå°‘ï¼Ÿ",    ### question in corresponding language version
  "answer": "$\\frac{5}{4}$",    ### ground truth
  "thinking_pred": "å—¯ï¼Œè¿™ä¸ªé—®é¢˜çœ‹èµ·æ¥æœ‰ç‚¹æŒ‘æˆ˜æ€§ï¼Œä¸è¿‡è®©æˆ‘æ…¢æ…¢æƒ³æƒ³ã€‚é¢˜ç›®æ˜¯è¯´ï¼Œåœ¨å¹³é¢ä¸Šæœ‰ä¸€ä¸ªç´§é›†C...",    ### Note: Model's thinking content. Note: If it is a non-reasoning model, leave this field blank.
  "answer_pred": "é¢˜ç›®è¦æ±‚åœ¨å¹³é¢ä¸Šçš„ä¸€ä¸ªç´§é›† \\( C \\)ï¼Œæ»¡è¶³å¯¹äºæ¯ä¸€ä¸ªæ–¹å‘ï¼Œ...",    ### Note: Model's answer content.
}
...
{"idx: 124, ...}
```

The complete file structure is as follows:

```shell
PolyMath/output
 â”œâ”€â”€ qwq-32b
 â”‚   â”œâ”€â”€ low
 â”‚   â”‚   â”œâ”€â”€ ar.jsonl
 â”‚   â”‚   â”œâ”€â”€ bn.jsonl
 â”‚   â”‚   â””â”€â”€ ... 
 â”‚   â”œâ”€â”€ medium
 â”‚   â”‚   â”œâ”€â”€ ar.jsonl
 â”‚   â”‚   â”œâ”€â”€ bn.jsonl
 â”‚   â”‚   â””â”€â”€ ... 
 â”‚   â”œâ”€â”€ high
 â”‚   â”‚   â”œâ”€â”€ ar.jsonl
 â”‚   â”‚   â”œâ”€â”€ bn.jsonl
 â”‚   â”‚   â””â”€â”€ ... 
 â”‚   â””â”€â”€ top
 â”‚       â”œâ”€â”€ ar.jsonl
 â”‚       â”œâ”€â”€ bn.jsonl
 â”‚       â””â”€â”€ ... 
 â”œâ”€â”€ deepseek-v3
 â”‚   â”œâ”€â”€ low
 â”‚   â”‚   â”œâ”€â”€ ar.jsonl
 â”‚   â”‚   â”œâ”€â”€ bn.jsonl
 â”‚   â”‚   â””â”€â”€ ... 
 â”‚   â”œâ”€â”€ medium
 â”‚   â”‚   â”œâ”€â”€ ar.jsonl
 â”‚   â”‚   â”œâ”€â”€ bn.jsonl
 â”‚   â”‚   â””â”€â”€ ... 
 â”‚   â”œâ”€â”€ high
 â”‚   â”‚   â”œâ”€â”€ ar.jsonl
 â”‚   â”‚   â”œâ”€â”€ bn.jsonl
 â”‚   â”‚   â””â”€â”€ ... 
 â”‚   â””â”€â”€ top
 â”‚       â”œâ”€â”€ ar.jsonl
 â”‚       â”œâ”€â”€ bn.jsonl
 â”‚       â””â”€â”€ ... 
 â””â”€â”€ ... (other models)
```

### Score Computation

The `/eval/run_eval.py` provides evaluation code for **accuracy** and **language consistency**. Please run `run_eval.sh` to iterate through your processed output files.

```
cd ../eval
bash run_eval.sh
```

`run_eval.sh`

```shell
model_list=(qwq-32b deepseek-v3)
language_list=(en zh ar bn de es fr id it ja ko ms pt ru sw te th vi)
level_list=(low medium high top)

for i in ${model_list[*]}; do
    for j in ${language_list[*]}; do
        for k in ${level_list[*]}; do
            python run_eval.py --model $i --language $j --level $k
        done
    done
done
```

You can customize `model_list`, `language_list`, and `level_list`. When it is detected that the evaluations for all levels of a particular model in a specific language are completed, the computation of the benchmark score will be triggered.

**During evaluation, a score file will be automatically generated at `/eval/output/{model_name}/score.json`, and all scores will be saved.**


## ğŸ“„ Citation

If you use **PolyMath** in your research or find our work useful, please cite us:

```bibtex
@article{wang2025polymath,
  title={PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts},
  author={Yiming Wang and Pei Zhang and Jialong Tang and Haoran Wei and Baosong Yang and Rui Wang and Chenshu Sun and Feitong Sun and Jiran Zhang and Junxuan Wu and Qiqian Cang and Yichang Zhang and Fei Huang and Junyang Lin and Fei Huang and Jingren Zhou},
  journal={arXiv preprint arXiv:2504.18428},
  year={2025},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2504.18428}, 
}
```
