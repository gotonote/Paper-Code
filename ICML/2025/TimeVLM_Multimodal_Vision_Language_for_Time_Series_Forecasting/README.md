# Time-VLM: Exploring Multimodal Vision-Language Models for Augmented Time Series Forecasting

<div align="center">

[![Paper](https://img.shields.io/badge/Paper-ArXiv-red.svg)](https://arxiv.org/abs/2502.04395)
[![Conference](https://img.shields.io/badge/Conference-ICML%202025-blue.svg)](https://icml.cc/virtual/2025/poster/44762)
[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.2.2+-green.svg)](https://pytorch.org/)
[![Stars](https://img.shields.io/github/stars/CityMind-Lab/ICML25-TimeVLM?style=social)](https://github.com/CityMind-Lab/ICML25-TimeVLM)

</div>

<div align="center">

![Framework Architecture](framework.png)

*Time-VLM Framework Architecture*

</div>

## ğŸ“– Overview

Time-VLM provides an extensible framework for integrating various Vision-Language Models (VLMs) with time series forecasting. It supports multiple VLM types (CLIP, BLIP2, ViLT) and enables flexible multimodal experiments.

## ğŸš€ Quick Start

### Environment Setup

To set up the environment, install Python 3.8 with Pytorch 1.4.4. Use the following commands for convenience:

```bash
conda create -n Time-VLM python=3.8
conda activate Time-VLM
pip install -r requirements.txt
```

### Dataset Preparation

Download the pre-processed datasets from:
- **Google Drive**: [Download Link](https://drive.google.com/drive/folders/13Cg1KYOlzM5C7K8gK8NfC-F3EYxkM3D2?usp=sharing)
- **Baidu Drive**: [Download Link](https://pan.baidu.com/s/1r3KhGd0Q9PJIUZdfEYoymg?pwd=i9iy)

Place the downloaded data in the `./dataset` folder.

### Running Experiments

Run the following scripts for different forecasting tasks:

```bash
# Long-term Forecasting (Full-shot, 100% data)
bash ./scripts/TimeVLM_long_1.0p.sh

# Long-term Forecasting (Few-shot, 10% data)
bash ./scripts/TimeVLM_long_0.1p.sh

# Short-term Forecasting
bash ./scripts/TimeVLM_short.sh

# Zero-shot Transfer Learning
bash ./scripts/TimeVLM_transfer.sh
```

> **âš ï¸ Important Notes**: 
> - Ensure you have downloaded the datasets and placed them in the correct directory
> - The default parameters provided in scripts are a good starting point, but you need to adjust them based on your specific dataset and requirements
> - **Script Naming Convention**: `TimeVLM_long_X.Xp.sh` where `X.Xp` indicates the percentage of data used (e.g., `1.0p` = 100%, `0.1p` = 10%)

## ğŸ“ Project Structure

```
Time-VLM/
â”œâ”€â”€ README.md                 # Project documentation
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ run.py                    # Main entry point for training and testing
â”œâ”€â”€ dataset/                  # Dataset directory
â”‚   â”œâ”€â”€ ETT/                  # ETT datasets
â”‚   â”œâ”€â”€ Weather/              # Weather dataset
â”‚   â”œâ”€â”€ Electricity/          # Electricity dataset
â”‚   â”œâ”€â”€ Traffic/              # Traffic dataset
â”‚   â””â”€â”€ ...
â”œâ”€â”€ scripts/                  # Training and evaluation scripts
â”‚   â”œâ”€â”€ TimeVLM_long_1.0p.sh # Long-term forecasting (full-shot, 100% data)
â”‚   â”œâ”€â”€ TimeVLM_long_0.1p.sh # Long-term forecasting (few-shot, 10% data)
â”‚   â”œâ”€â”€ TimeVLM_short.sh     # Short-term forecasting
â”‚   â”œâ”€â”€ TimeVLM_transfer.sh  # Zero-shot transfer learning
â”‚   â””â”€â”€ ...
â”œâ”€â”€ src/                      # Source code
â”‚   â”œâ”€â”€ TimeVLM/             # Time-VLM model implementation
â”‚   â”‚   â”œâ”€â”€ model.py         # Main model architecture
â”‚   â”‚   â”œâ”€â”€ vlm_custom.py    # Custom VLM implementations
â”‚   â”‚   â”œâ”€â”€ vlm_manager.py   # VLM manager for different types
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ utils/                # Utility functions
â”‚   â”œâ”€â”€ models/               # Model implementations
â”‚   â”œâ”€â”€ layers/               # Custom layers
â”‚   â””â”€â”€ ...
â”œâ”€â”€ exp/                      # Experiment configurations
â”œâ”€â”€ logs/                     # Training logs
â”œâ”€â”€ ts-images/               # Generated time series images
â””â”€â”€ ...
```

## âš™ï¸ Configuration & Tuning

### Core Parameters

| Parameter | Default | Range | Description |
|:---------:|:-------:|:-----:|:------------|
| **`d_model`** | `128` | `32-512` | **Most Important**: Model dimension |
| **`dropout`** | `0.1` | `0.1-0.5` | Dropout rate |
| **`learning_rate`** | `0.001` | `0.0001-0.01` | Learning rate |
| **`batch_size`** | `32` | `-` | Adjust based on GPU memory |
| **`image_size`** | `56` | `28-112` | Time series image size |
| **`periodicity`** | `24` | `-` | Data periodicity for image generation |
| **`norm_const`** | `0.4` | `0.1-1.0` | Normalization constant |

### Script Parameters

| Parameter | Default | Description |
|:---------:|:-------:|:------------|
| **`percent`** | `1.0` | Data usage ratio |
| **`vlm_type`** | `clip` | VLM type [clip, blip2, vilt, custom] |
| **`image_size`** | `56` | Time series image size (28-224) |
| **`periodicity`** | `24` | Data periodicity for image generation |
| **`use_mem_gate`** | `True` | Memory fusion gate |
| **`finetune_vlm`** | `False` | Finetune pre-trained VLM |
| **`three_channel_image`** | `True` | Generate RGB images |
| **`learnable_image`** | `True` | Learnable image generation |


## ğŸ“š Citation

If you find this repository useful, please cite our paper:

```bibtex
@inproceedings{zhong2025time,
  title={Time-VLM: Exploring Multimodal Vision-Language Models for Augmented Time Series Forecasting},
  author={Zhong, Siru and Ruan, Weilin and Jin, Ming and Li, Huan and Wen, Qingsong and Liang, Yuxuan},
  booktitle={Proceedings of the 42nd International Conference on Machine Learning},
  year={2025}
}
```